{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce192710-a2c3-4c45-b60b-66c50a1e2bc9",
   "metadata": {},
   "source": [
    "# Creating a fully connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d597bd1-fb92-4601-9092-eea1fe242619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "059d5106-18f7-4aa4-a1a2-0cb78a274f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Using device: cuda:7\n"
     ]
    }
   ],
   "source": [
    "# Selection of a GPU\n",
    "gpu_num = -1\n",
    "\n",
    "#Loop through avtive GPUs to check for free GPUs\n",
    "for i in range(7, 0, -1):\n",
    "    print(i)\n",
    "    result = subprocess.run(f\"nvidia-smi -i {i}\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    if \"No running processes found\" in result.stdout:\n",
    "        gpu_num = i\n",
    "        break\n",
    "\n",
    "#If no GPU found, only run nvidia-smi once\n",
    "first_attempt = True\n",
    "\n",
    "#Ensure correct input\n",
    "while 0 > gpu_num or gpu_num > 7:\n",
    "    #Run nvidia-smi command\n",
    "    if first_attempt:\n",
    "        !nvidia-smi\n",
    "        first_attempt = False\n",
    "\n",
    "    #Type checking for int between 0 and 7\n",
    "    try:\n",
    "        gpu_num = int(input(\"No Free GPUs Found, manually enter GPU index (0-7): \"))\n",
    "        if 0 <= gpu_num <= 7:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Error: GPU index must be between 0 and 7.\")\n",
    "    except ValueError:\n",
    "        print(\"Error: Please enter a valid integer.\")\n",
    "\n",
    "#Declare device as free device found above  \n",
    "device = torch.device(f'cuda:{gpu_num}' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "affd2dad-2489-4ef0-9e51-99c19a734038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Separate fully connected layers for each feature set\n",
    "        self.fc1_1 = nn.Linear(input_size, input_size // 2)\n",
    "        self.fc1_2 = nn.Linear(input_size, input_size // 2)\n",
    "        self.fc1_3 = nn.Linear(input_size, input_size // 2)\n",
    "\n",
    "        # Trainable weights for weighted summation\n",
    "        self.weight1 = nn.Parameter(torch.tensor(1.0))\n",
    "        self.weight2 = nn.Parameter(torch.tensor(1.0))\n",
    "        self.weight3 = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "        # Combined fully connected layers\n",
    "        self.fc2 = nn.Linear((input_size // 2), input_size // 4)\n",
    "        self.fc3 = nn.Linear(input_size // 4, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        # Apply separate fully connected layers to each feature set\n",
    "        x1 = self.fc1_1(x1)\n",
    "        x2 = self.fc1_2(x2)\n",
    "        x3 = self.fc1_3(x3)\n",
    "\n",
    "        # Weighted summation of feature sets\n",
    "        x = self.weight1 * x1 + self.weight2 * x2 + self.weight3 * x3\n",
    "\n",
    "        # Pass through additional fully connected layers\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b2d2060-0545-4f8f-8f9f-6ecc201e0d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, patience):\n",
    "    time_start = time()\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    final_epoch = -1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for features1, features2, features3, labels in train_loader:\n",
    "            labels = labels.type(torch.LongTensor).to(device)\n",
    "            features1, features2, features3 = features1.to(device), features2.to(device), features3.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(features1, features2, features3)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()           # reset gradient\n",
    "            loss.backward()                 # automated backwards pass\n",
    "            optimizer.step()                # take a step\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        epoch_losses.append(avg_loss)  # Store the average loss\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features1, features2, features3, labels in val_loader:\n",
    "                labels = labels.type(torch.LongTensor).to(device)\n",
    "                features1, features2, features3 = features1.to(device), features2.to(device), features3.to(device)\n",
    "                outputs = model(features1, features2, features3)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')\n",
    "        final_epoch = epoch+2\n",
    "\n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(colored(f'Early stopping triggered after {epoch+1} epochs.', 'red'))\n",
    "                break\n",
    "\n",
    "    # Load the best model state\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    time_stop = time()\n",
    "    time_elapsed = time_stop - time_start\n",
    "    print(f'Elapsed time {round(time_elapsed, 1)} sec.')\n",
    "\n",
    "    # Plotting the loss over epochs\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, final_epoch), epoch_losses, marker='o')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.xticks(range(1, epochs + 1))\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the validation error over epochs\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, final_epoch), val_losses, marker='o')\n",
    "    plt.title('Validation Error over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Error')\n",
    "    plt.xticks(range(1, epochs + 1))\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9441b1db-5941-4fa3-ac36-df2a98fd4e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop with confusion matrix\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images1, images2, images3, labels in test_loader:\n",
    "            images1, images2, images3, labels = images1.to(device), images2.to(device), images3.to(device), labels.to(device)\n",
    "            outputs = model(images1, images2, images3)\n",
    "            _, predicted = torch.max(outputs.data, 1)  # predicted = index of maximum probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Append true and predicted labels for confusion matrix\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate and print accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy of the model on the test dataset: {accuracy:.2f}%')\n",
    "\n",
    "    # Generate and plot confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a99b16a-4b4d-4d69-baa1-23b17171f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModel(file_name, batch_size=512, epochs=25, lrate=0.01, patience=16):\n",
    "    # Load three separate feature sets\n",
    "    data1 = np.load(f\"Features/{file_name}_Normal.npz\")\n",
    "    features1 = data1['features']\n",
    "    print('data1 loaded...')\n",
    "    data2 = np.load(f\"Features/{file_name}_Fourier.npz\")\n",
    "    features2 = data2['features']\n",
    "    print('data2 loaded...')\n",
    "    data3 = np.load(f\"Features/{file_name}_HighPass.npz\")\n",
    "    features3 = data3['features']\n",
    "    print('data3 loaded...')\n",
    "    labels = data1['labels']\n",
    "    print('labels loaded...')\n",
    "\n",
    "    print(f\"Features1 shape: {features1.shape}\")\n",
    "    print(f\"Features2 shape: {features2.shape}\")\n",
    "    print(f\"Features3 shape: {features3.shape}\")\n",
    "    print(f\"Labels shape:    {labels.shape}\")\n",
    "\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    _, test_indices = train_test_split(\n",
    "        range(len(labels)), test_size=0.2, random_state=0, stratify=labels\n",
    "    )\n",
    "    mask = np.ones(len(labels), dtype=bool)\n",
    "    mask[test_indices] = False\n",
    "    train_indices_pre = np.arange(len(labels))[mask]\n",
    "    X1_train_pre, X1_test = features1[train_indices_pre], features1[test_indices]\n",
    "    X2_train_pre, X2_test = features2[train_indices_pre], features2[test_indices]\n",
    "    X3_train_pre, X3_test = features3[train_indices_pre], features3[test_indices]\n",
    "    y_train_pre, y_test = labels[train_indices_pre], labels[test_indices]\n",
    "\n",
    "    print('Test split complete...')\n",
    "    \n",
    "    # Split the training data further into train and validation sets\n",
    "    _, val_indices = train_test_split(\n",
    "        range(len(y_train_pre)), test_size=0.25, random_state=0, stratify=y_train_pre\n",
    "    )\n",
    "    mask = np.ones(len(y_train_pre), dtype=bool)\n",
    "    mask[val_indices] = False\n",
    "    train_indices = np.arange(len(y_train_pre))[mask]\n",
    "    X1_train, X1_val = X1_train_pre[train_indices], X1_train_pre[val_indices]\n",
    "    X2_train, X2_val = X2_train_pre[train_indices], X2_train_pre[val_indices]\n",
    "    X3_train, X3_val = X3_train_pre[train_indices], X3_train_pre[val_indices]\n",
    "    y_train, y_val = y_train_pre[train_indices], y_train_pre[val_indices]\n",
    "\n",
    "    print('Validation split complete...')\n",
    "\n",
    "    print()\n",
    "    print(f\"Example entry: X1 train   {X1_train_pre[0]}\")\n",
    "    print(f\"               Features1  {features1[train_indices_pre[0]]}\")\n",
    "    print(f\"               X2 train   {X2_train_pre[0]}\")\n",
    "    print(f\"               Features2  {features2[train_indices_pre[0]]}\")\n",
    "    print(f\"               X3 train   {X3_train_pre[0]}\")\n",
    "    print(f\"               Features3  {features3[train_indices_pre[0]]}\")\n",
    "    print(f\"               Y          {y_train_pre[0]}\")\n",
    "    print(f\"               Label      {labels[train_indices_pre[0]]}\")\n",
    "    print()\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X1_train_tensor = torch.tensor(X1_train, dtype=torch.float32)\n",
    "    X2_train_tensor = torch.tensor(X2_train, dtype=torch.float32)\n",
    "    X3_train_tensor = torch.tensor(X3_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "    X1_val_tensor = torch.tensor(X1_val, dtype=torch.float32)\n",
    "    X2_val_tensor = torch.tensor(X2_val, dtype=torch.float32)\n",
    "    X3_val_tensor = torch.tensor(X3_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    X1_test_tensor = torch.tensor(X1_test, dtype=torch.float32)\n",
    "    X2_test_tensor = torch.tensor(X2_test, dtype=torch.float32)\n",
    "    X3_test_tensor = torch.tensor(X3_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    print('Converted to tensor...')\n",
    "\n",
    "    # Create PyTorch Datasets and DataLoaders\n",
    "    train_dataset = TensorDataset(X1_train_tensor, X2_train_tensor, X3_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X1_val_tensor, X2_val_tensor, X3_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X1_test_tensor, X2_test_tensor, X3_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print('Created loaders...')\n",
    "\n",
    "    # Set up model, loss, and optimizer\n",
    "    input_size = X1_train.shape[1]  # assuming all three have the same shape\n",
    "    num_classes = len(np.unique(labels))\n",
    "    model = NeuralNet(input_size=input_size, output_size=num_classes).to(device)\n",
    "    \n",
    "    print(colored(\"Neural Net Definition:\", \"blue\"))\n",
    "    print(f\"{model}\\n\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lrate)\n",
    "\n",
    "    # Train and test the model\n",
    "    print(colored(\"Begin Model Training:\", \"blue\"))\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, epochs=epochs, patience=patience)\n",
    "    test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29657248-2a0a-4603-aeb4-f754f522b91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1 loaded...\n",
      "data2 loaded...\n",
      "data3 loaded...\n",
      "labels loaded...\n",
      "Features1 shape: (185015, 32768)\n",
      "Features2 shape: (185015, 32768)\n",
      "Features3 shape: (185015, 32768)\n",
      "Labels shape:    (185015,)\n",
      "Test split complete...\n"
     ]
    }
   ],
   "source": [
    "runModel(file_name='256_AI_And_Classes_features_VGG', batch_size=512, epochs=50, lrate=0.001, patience=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2551e57-3deb-4676-9ba4-a478cd2ae9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda_env",
   "language": "python",
   "name": "anaconda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

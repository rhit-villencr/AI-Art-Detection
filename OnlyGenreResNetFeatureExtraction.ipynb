{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf30b7e7-9fdb-4e3e-bcfe-5053c410a016",
   "metadata": {},
   "source": [
    "Feature extraction for classifying only genre (using new dataset from wikiart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b5a133-54aa-48d2-a35f-04ca87f0b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908ec6de-71e8-4251-b4e6-a5eea6d5ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input for image resolution\n",
    "px = 256\n",
    "img_size = (px,px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf9f70f-35a4-4095-b111-cec09a65af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the dataset directory\n",
    "data_file = \"\"  # Replace with the actual path to your dataset\n",
    "data_dir = f\"Datasets/{data_file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878964f1-01fb-42f0-87ef-fb58776d5a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data \n",
    "\n",
    "# Define function that plots multiple images in an array of images\n",
    "def PlotImages(ix_start,num_img,rows,cols,images,labels,label_names,color=True):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplots_adjust(wspace=0.5,hspace=0.5)\n",
    "    for i in range(num_img):\n",
    "        plt.subplot(rows,cols,i+1)\n",
    "        plt.axis('off')\n",
    "        if color:\n",
    "            plt.imshow(images.permute(0, 2, 3, 1)[ix_start+i])\n",
    "        else: \n",
    "            plt.imshow(np.squeeze(images[ix_start+i]),cmap='Greys')\n",
    "        plt.title(label_names[labels[ix_start+i]])\n",
    "\n",
    "# Image loader and scaler for image exploration\n",
    "batch_size = 25\n",
    "load_image = transforms.Compose([\n",
    "    transforms.Resize((px, px)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "dataset = ImageFolder(root=data_dir, transform=load_image)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "data_iter = iter(dataloader)\n",
    "label_names = dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d222908-b347-4528-8c8f-3b203ff4a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(data_iter)\n",
    "next(data_iter)\n",
    "# PlotImages(0,batch_size,5,5,images,labels,label_names,color=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cf94f5-bd48-4751-a810-83dc8131a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract features using ResNet\n",
    "def extract_features(data_dir, img_size, batch_size=512, device='cuda:1' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    Extract features from images using the convolutional layers of the VGG16 network.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Directory containing the image dataset. Each class should have its own subfolder.\n",
    "        img_size (tuple): The resolution (width, height) of the images to resize.\n",
    "        batch_size (int): Number of images to process in one batch.\n",
    "        device (str): The device to run the model on, 'cuda' or 'cpu'.\n",
    "        \n",
    "    Returns:\n",
    "        features (numpy.ndarray): Extracted features from the images.\n",
    "        labels (list): Corresponding class labels for each extracted feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the pre-trained VGG16 model\n",
    "    resnet = models.resnet152(weights='ResNet152_Weights.DEFAULT')\n",
    "\n",
    "    # Only use the convolutional layers (remove the fully connected layers)\n",
    "    # VGG16 features have conv layers + pooling\n",
    "    resnet_features = nn.Sequential(*list(resnet.children())[:-1]).to(device)\n",
    "\n",
    "    # Disable gradient calculations since we're only using the model for feature extraction\n",
    "    for param in resnet_features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Define the image transformation pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(img_size),      # Resize to the specified resolution\n",
    "        transforms.ToTensor(),            # Convert images to PyTorch tensors\n",
    "        transforms.Normalize(             # Normalize images to match VGG16 input requirements\n",
    "            mean=[0.485, 0.456, 0.406],   # These are the mean values for VGG16's training data (ImageNet)\n",
    "            std=[0.229, 0.224, 0.225]     # These are the standard deviations for ImageNet\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # Load the dataset, where each class is in its own subfolder\n",
    "    dataset = ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "    # Create a DataLoader to handle batch processing of images\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Lists to store extracted features and corresponding labels\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Iterate through the dataset in batches\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)  # Send images to the specified device (GPU or CPU)\n",
    "\n",
    "        # Pass images through the VGG16 convolutional layers\n",
    "        features = resnet_features(images)\n",
    "\n",
    "        # Flatten the features (we take the output from the last conv layer)\n",
    "        features = features.view(features.size(0), -1)\n",
    "\n",
    "        # Move the features back to CPU (to prepare for saving)\n",
    "        features = features.cpu().detach().numpy()\n",
    "\n",
    "        # Append the extracted features and labels\n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels.numpy())  # Labels are already on CPU, so just append them\n",
    "\n",
    "    # Concatenate all features and labels from the batches into single arrays\n",
    "    all_features = np.concatenate(all_features, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    return all_features, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4998aaa5-0ee8-4970-884d-08269df97764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "features, labels = extract_features(data_dir, img_size)\n",
    "\n",
    "# Save the extracted features and labels as a .npz file\n",
    "save_file = f\"Features/{str(px)}_{data_file}_only_genre_features_resnet.npz\"\n",
    "np.savez(save_file, features=features, labels=labels)\n",
    "\n",
    "print(f\"Features and labels extracted and saved to '{save_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bbf432-e49a-4e60-a48e-8684522ecdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

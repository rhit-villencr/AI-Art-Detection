{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a86829b2-04da-41a5-a9e7-f34b4ec5e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (optional) Change padding \"same\" --> \"valid\". It will change sizes to be smaller yet. (4x4 instead of 7x7 in those 2 places)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7817db-962c-4ced-9de6-12ca3b5b6215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n",
      "Device name: Quadro RTX 6000\n"
     ]
    }
   ],
   "source": [
    "# Define the device (use GPU if available)\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Print the device type\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# If using a CUDA device, print the name of the CUDA device\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(device.index)\n",
    "    print(f'Device name: {device_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f73d9126-063b-45a4-a031-4bf67f4690d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1000 samples\n",
      "Testing set size: 30000 samples\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "# Transformation for the images (resize, normalization, etc.)\n",
    "img_size = 256\n",
    "batch_size = 512\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)), # Resizing to 28x28 pixels as in the original MNIST format\n",
    "    transforms.ToTensor(),         # Convert image to tensor\n",
    "    transforms.Normalize(          # Normalize with ImageNet mean and std\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Assuming your images are already saved in appropriate directories\n",
    "# Replace the path with where your Fashion MNIST images are stored\n",
    "train_dataset = datasets.ImageFolder(root=\"Small_Just_Classes_Dataset/train\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=\"Small_Just_Classes_Dataset/test\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_classes = len(train_loader.dataset.classes)\n",
    "\n",
    "# Display dataset information\n",
    "print(f'Training set size: {len(train_loader.dataset)} samples')\n",
    "print(f'Testing set size: {len(test_loader.dataset)} samples')\n",
    "print(f'Number of classes: {num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d7863de-66da-41a5-be53-68c1ebafec5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m         plt\u001b[38;5;241m.\u001b[39mimshow(image_np)\n\u001b[1;32m     19\u001b[0m         plt\u001b[38;5;241m.\u001b[39mtitle(label_names[labels[i]])\n\u001b[0;32m---> 21\u001b[0m PlotImages(num_img\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, loader\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_loader\u001b[49m, label_names\u001b[38;5;241m=\u001b[39mtrain_loader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mclasses)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Define function that plots multiple images from a DataLoader\n",
    "def PlotImages(num_img, rows, cols, loader, label_names, color=True):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    images, labels = [], []\n",
    "    for batch_images, batch_labels in loader:\n",
    "        for i in range(len(batch_images)):\n",
    "            images.append(batch_images[i])\n",
    "            labels.append(batch_labels[i].item())\n",
    "            if len(images) >= num_img:\n",
    "                break\n",
    "        if len(images) >= num_img:\n",
    "            break\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.axis('off')\n",
    "        image_np = image.permute(1, 2, 0).numpy()\n",
    "        plt.imshow(image_np)\n",
    "        plt.title(label_names[labels[i]])\n",
    "\n",
    "PlotImages(num_img=16, rows=4, cols=4, loader=train_loader, label_names=train_loader.dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f1b244-8505-4b2e-9d02-02a098fd0714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
      "  (pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
      "  (pool2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (fc1): Linear(in_features=65536, out_features=240, bias=True)\n",
      "  (fc2): Linear(in_features=240, out_features=120, bias=True)\n",
      "  (fc3): Linear(in_features=120, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the LeNet model\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding='same')\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding='same')\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(in_features=16*int(img_size/4)*int(img_size/4), out_features=240)\n",
    "        self.fc2 = nn.Linear(in_features=240, out_features=120)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.shape[0], 16*int(img_size/4)*int(img_size/4))\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "model = LeNet().to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b451e3ec-79aa-43c3-8796-57042342b443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, loss function, and optimizer are set up.\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print('Model, loss function, and optimizer are set up.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7461b900-2694-4708-a0ed-3f902abb5bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 2.3025\n",
      "Epoch [2/15], Loss: 2.3005\n",
      "Epoch [3/15], Loss: 2.2988\n",
      "Epoch [4/15], Loss: 2.2971\n",
      "Epoch [5/15], Loss: 2.2953\n",
      "Epoch [6/15], Loss: 2.2936\n",
      "Epoch [7/15], Loss: 2.2918\n",
      "Epoch [8/15], Loss: 2.2900\n",
      "Epoch [9/15], Loss: 2.2879\n",
      "Epoch [10/15], Loss: 2.2858\n",
      "Epoch [11/15], Loss: 2.2835\n",
      "Epoch [12/15], Loss: 2.2807\n",
      "Epoch [13/15], Loss: 2.2776\n",
      "Epoch [14/15], Loss: 2.2742\n",
      "Epoch [15/15], Loss: 2.2703\n",
      "elapsed time 726.9 sec.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    time_start = time()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #TODO: Early stopping\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "    time_stop = time()\n",
    "    time_elapsed = time_stop - time_start\n",
    "    print(f'elapsed time {round(time_elapsed,1)} sec.')\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafeb62a-fc1e-4f3b-92b2-1d1b94b1254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "print(f'Test Accuracy of the model on the test dataset: {test_model(model, test_loader):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e34df-7bf8-4a82-9a5a-e4bc08a851f0",
   "metadata": {},
   "source": [
    "| Device | Batch Size | Image Size | Accuracy | Time (sec) | Epochs | Test                  |\n",
    "|:-------|:-----------|:-----------|:---------|:-----------|:-------|:----------------------|\n",
    "| GPU    | 512        | 256x256    | 0.92     | 27700      | 25     | AI                    |\n",
    "| GPU    | 512        | 256x256    | 0.46     | 4834.9     | 5      | Genres                |\n",
    "| GPU    | 512        | 128x128    | 0.41     | 4485.4     | 5      | Genres                |\n",
    "| GPU    | 512        | 256x256    | 0.18     | 726.9      | 15     | Genres - Small Dataset|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725aae93-15e9-451b-acb5-7072b282515c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-env",
   "language": "python",
   "name": "project-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
